---
title: "P8106 Final Project"
author: "Phoebe Mo(km3624), Stella Li(cl4043), Yihan Feng()"
date: "5/7/2021"
output:
  pdf_document:
    latex_engine: xelatex
---


```{r warning=FALSE, message=FALSE}
library(tidyverse)
library(RColorBrewer)
library(factoextra)
library(RColorBrewer)
library(gplots)
library(randomForest)
library(e1071)
library(ranger)
library(gbm)
library(xgboost)
library(lattice)
library(caret)
library(corrplot)
library(mgcv)
```

# 1. INTRODUCTION

## 1.1 Motivation and Objective

Pokemon are fictional creatures, which can be captured by players and trained to battle each other in the augmented reality game PokÃ©mon Go!. In this project, we use a dataset from Kaggle that contains different attributes and catch rates for 721 unique Pokemons. We try to understand the relationship between Pokemons' different attributes and their respective catch rates. Here are some questions we want to answer: Which predictor(s) play important roles in predicting catch rates? Which type of model (linear or non-linear) serves as a better method to predict the catch rate?

## 1.2 Data Preparation and Cleaning

The original dataset has 20 predictors such as HP, and the outcome 'catch_rate'. After cleaning the names of these variables, I did the following steps to clean the data:

1.2.1 Notice that 'type_2' and 'egg_group_2' are indicators of if a pokemon has a second type or belongs to a second egg group. The 'Null' values in the data means the pokemon does not has the second type/group, so I changed them into "none" to make them as a category to be meaningful;

1.2.2 The 'generation' predictor is originally a numeric type, but it has only 6 integer values, so I decided to mutate it to be categorical;

1.2.3. Irrelevant variables 'number' and 'name' are dropped. 'total', which is the total base battle statistic for each pokemon, is calculated and reflected in other battle attributes such as 'hp' and 'attack'. So I dropped it to avoid intercollinearity. Later, after plotting the correlation map, I found 'weight_kg' and 'height_m' has relatively high correlation(correlation plot is shown in section 2). After consideration,  I chose to drop 'height_m' since it may has less effect on catch rate compared to 'weight_kg'. 'has_gender' is also dropped because all values are "TRUE".

```{r message=FALSE, warning=FALSE}
# data input
pkmn.df = read.csv("./pokemon.csv") %>%
  janitor::clean_names() %>%
  mutate(type_2 = replace_na(type_2, "none"),
         egg_group_2 = replace_na(egg_group_2, "none")
         ) %>%
  dplyr::select(-number, -total, -name, -has_gender) %>%
  na.omit()

factor.cols = c("type_1", "type_2", "is_legendary", "color", "egg_group_1", "egg_group_2", "has_mega_evolution", "body_style")

pkmn.df = pkmn.df %>% 
  mutate_at(factor.cols, funs(factor(.)))

# set training and testing
trRows = createDataPartition(pkmn.df$catch_rate,
                             p = 0.75,
                             list = FALSE)
```

# 2. Exploratory Analysis / Visualization

After plotting scatter plots for continuous predictors and boxplots for categorical predictors, significant trends were not observed in different types, egg groups, body_styles or colors. However, the battle attributes: 'hp', 'attack', 'defense', 'sp_atk', 'sp_def', and 'speed' seem to have a negative association with catch rate. The 'weight_kg' also seems to have negative association with catch rate. And the pokemons that are legendary have significantly lower catch rates. Below are some selected visualizations:

```{r message=FALSE, warning=FALSE}
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5) 
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1) 
theme1$plot.line$lwd <- 2 
theme1$strip.background$col <- rgb(.0, .2, .6, .2) 
trellis.par.set(theme1)
featurePlot(x = pkmn.df[, 3:8], y = pkmn.df$catch_rate,
            plot = "scatter",
            span = .5,
            labels = c("Battel Feature Predictors","Catch Rate"), type = c("p", "smooth"), layout = c(2,3))


# reread the data, since we need to use the 'total' variable here for visualization
pkmn.df.vis = read_csv("./pokemon.csv") %>% janitor::clean_names()
total.plot = ggplot(pkmn.df.vis, aes(x = total, y = catch_rate)) + 
  geom_point() +
  labs(title = "Figure 2. Scatterplot of total battle features vs. catch rate")
total.plot

# checking correlation between continuous predictors
# height_m and weight_kg have high correlation = 0.63. So going to drop one of them, here I choose to drop height_m.
cor(pkmn.df %>% dplyr::select(-c(type_1, type_2, generation, is_legendary, color, egg_group_1, egg_group_2, body_style, has_mega_evolution))) %>%
  corrplot(method = "circle", type = "upper", diag = F)
```

# 3. Models

The dataset for analysis contains 644 samples and 18 predictors: 

* `type_1`
* `typw_2 `
* `hp`: health point
* `attack`
* `defense`
* `sp_atk`: attack speed
* `sp_def`: defense speed
* `speed`
* `generation`
* `is_legendary`
* `color`
* `pr_male`: probability of being a male Pokemon
* `egg_group_1`
* `egg_group_2`
* `has_mega_evolution`
* `height_m`
* `weight_kg`
* `body_style`
* `catch_rate`: the response (on 0 - 255 scale)


The dataset is split into 75:25 training to test dataset. Based on our midterm project, We include GAM model, since it has a dominant performance among some other methods(e.g. lda, qda, nb, lasso, ridge...), as a competitive model for predicting the catch rate. We also choose Boosting, Random Forest, Bagging, and Support Vector Machine models to train the data with cross validation. RMSE is used to compare and choose the final model. Finally, we use clustering to have a better visualization of pokemons that have similar attributes and to see if significant relationship between combat attributes and catch rate can be found in it.


## 3.1 GAM

Based on the visualization plots, we can see that some continuous variables have curve-like trend in the right tail. In this case, I consider using GAM model in order to include this trend into the analysis. For such predictors, I allow GAM to make it non-parametric smoothing term. GCV is used to choose the degree of freedom for the model. 

After fitting, it is observed that the smooth terms of 'hp', 'attack', 'defense', 'sp_atk', 'speed', and the coefficients of 'type_1Poison', 'type_2Water', 'generation3', 'pr_male', 'egg_group_1Undiscovered', 'body_stylemultiple_bodies', are significant. These result greatly coincides with the ones given by previous model.

```{r}
set.seed(1)
gam_fit = gam(catch_rate ~ type_1 + type_2 + s(hp) + s(attack) + s(defense) + s(sp_atk) + s(sp_def) + s(speed) + generation + is_legendary + color + pr_male + egg_group_1 + egg_group_2 + has_mega_evolution + s(weight_kg) + body_style, data = pkmn.df[trRows, ])

#gam_plot = plot(gam_fit, scale = 0)
gam.rmse = sqrt(mean((pkmn.df$catch_rate[trRows] - predict(gam_fit, newdata = pkmn.df[trRows, ])) ^ 2)) # gam.rmse = 42.10908
```


## 3.2 Bagging, Boosting, Random Forest

We decide to use tree-based ensemble methods for regression.

```{r}
# Boosting
ctrl <- trainControl(method = "cv") 

gbm.grid <- expand.grid(n.trees = c(2000,3000,4000),
                        interaction.depth = 1:4,
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = c(1,10))
set.seed(1)
gbm.fit <- train(catch_rate ~ ., 
                 pkmn.df[trRows,], 
                 method = "gbm",
                 tuneGrid = gbm.grid,
                 trControl = ctrl,
                 verbose = FALSE)

gbm.pred = predict(gbm.fit, newdata = pkmn.df[trRows,])
gbm.rmse = RMSE(gbm.pred, pkmn.df$catch_rate[trRows])
gbm.rmse # gbm.rmse = 38.38418

# ggplot(gbm.fit, highlight = TRUE)
```

```{r}
# Random Forest
set.seed(2021)
rf.fit = randomForest(catch_rate ~ .,
                      pkmn.df,
                      subset = trRows,
                      mtry = 6)

rf.pred = predict(rf.fit, newdata = pkmn.df[trRows,])
rf.rmse = RMSE(rf.pred, pkmn.df$catch_rate[trRows])


# Bagging
set.seed(2021)
bagging <- randomForest(catch_rate ~ .,
                        pkmn.df,
                        subset = trRows,
                        mtry = 19)

# fast implementation
set.seed(2021)
rf2.fit <- ranger(catch_rate ~ .,
              pkmn.df[trRows,],
              mtry = 6) 

pred.rf2 <- predict(rf2.fit, data = pkmn.df[trRows,])$predictions
rf.rmse2 = RMSE(pred.rf2, pkmn.df$catch_rate[trRows])
rf.rmse2
```

## 3.3 SVM

In SVM, we want to find a separating hyperplane that maximizes the gap between classes to optimally predict the outcome. Support Vector Machine with both linear and radial kernel is used to train the data. The tuning parameter epsilon(trade-off between correct classification and maximization of the gap) and gamma(how non-linear the fit could be) are tuned given a suitable range of numbers in the function.

```{r}
# linear kernel
set.seed(2021)
svml.fit = tune.svm(catch_rate ~ .,
                    data = pkmn.df[trRows,],
                    kernel = "linear",
                    epsilon = exp(seq(-5, 0, len = 20)))

svml.pred = predict(svml.fit$best.model, newdata = pkmn.df[trRows,])
svml.rmse = RMSE(svml.pred, pkmn.df$catch_rate[trRows])


# radial kernel
set.seed(2021)
svmr.fit = tune.svm(catch_rate ~. ,
                    data = pkmn.df[trRows,],
                    kernel = "radial",
                    epsilon = exp(seq(-5, 0, len = 20)),
                    gamma = exp(seq(-6, -2, len = 10)))

svmr.pred = predict(svmr.fit$best.model, newdata = pkmn.df[trRows,])
svmr.rmse = RMSE(svmr.pred, pkmn.df$catch_rate[trRows])
```

## 3.4 Clustering

In order to have a better visualization of the relationship between the pokemon and their different attributes, we use both k-means clustering and hierarchical clustering to group similar pokemons together. Here, we use only numerical variables.
As can see from k-means clustering, we use the optimal number of clusters = 3 and use Euclidean distance to decide the similarity. From the plot briefly, cluster 1 tends to has lower hp, cluster 2 has lower catch_rate, and cluster 3 has relatively higher weight and defense.
For the hierarchical clustering, we use complete linkage(maximal inter-cluster dissimilarity) and euclidean distance. Since hierarchical clustering provides us all the possible combinations of clusters, to visualize, we made a heat map to see how all the variables vary for each observation. From the heat map, we found pokemons that have higher hp, height, and weight tend to have lower catch_rate, whereas those which have relatively low combat attributes tend to have higher catch_rate.

```{r}
# reorganize data to be used
poke_cluster = read_csv("./pokemon.csv") %>%
  janitor::clean_names() %>%
  dplyr::select(hp, attack, defense, sp_atk, sp_def, speed, height_m, weight_kg, catch_rate) %>%
  na.omit() %>%
  scale()

# k-mean clustering, using optimal number of clusters = 3
optimal_cluster = fviz_nbclust(poke_cluster, FUNcluster = kmeans, method = "silhouette")
set.seed(1)
cluster_km = kmeans(poke_cluster, centers = 3, nstart = 30)
cluster_km_vis = fviz_cluster(list(data = poke_cluster, cluster = cluster_km$cluster),
                              ellipse.type = "convex",
                              geom = c("point", "text"),
                              labelsize = 5,
                              palette = "Dark2") + labs(title = "K-means Clustering")

# show k-means clustering plot
cluster_km_vis

# hierarchical clustering
col1 = colorRampPalette(brewer.pal(9, "PRGn"))(100)

# show hierarchical clustering heat map
heatmap.2(t(poke_cluster),
          col = col1, keysize = 0.9, key.par = list(cex = 0.5),
          trace = "none", key = TRUE, cexCol = 0.75,
          margins = c(10, 10))

# check for different clusters' members
hc_complete = hclust(dist(poke_cluster), method = "complete")
hc_dendrogram = fviz_dend(hc_complete, k = 8, cex = 0.3, palette = "jco",
             color_labels_by_k = TRUE,
             rect = TRUE, rect_fill = TRUE, rect_border = "jco",
             labels_track_height = 2.5)

# for example, for group_8 members, they have high weight, high height, high hp, and have other high attributes relatively
group_8 = poke_cluster[cutree(hc_complete, 8) == 8, ]
```

## 3.5 Variable Importance

```{r}

```

## 3.6 Final Model and Visualization

```{r}
final.model.table = data.frame(model = rep(c("Boosting", "Random Forest", "Bagging", "SVM (linear)", "SVM (radial)")),
                               RMSE = rep(c(rf.rmse, rf.rmse, rf.rmse2, svml.rmse, svmr.rmse))) %>%
  knitr::kable()
final.model.table
```


# 4. Conclusions


# 5. Limitations

5.1 The GAM model does not truly select the tuning parameter and do the model training, therefore this may be the limitation in the GAM model. However, since it has the smallest training RMSE, it is still chosen as our best method to do the prediction in this project.

5.2 For the k-means clustering, since we need to decide k ourselves, there is no reference for us to decide which k is better. So the manually selected k and the given visualization may not be the best presentation.


